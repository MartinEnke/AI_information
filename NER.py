import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Tony Kross visited Athens in August 2012.")
for ent in doc.ents:
  print(ent.text, ent.label_)




"""
Try to use the given code to find all named entities in the â€œpreprocessed_romeo_and_juliet.txtâ€ with the DATE label.
"""
import spacy

# Load SpaCy's English language model
nlp = spacy.load("en_core_web_sm")

# Load the preprocessed text
with open("romeo_and_juliet.txt", "r", encoding="utf-8") as file:
    text = file.read()

# Process the text with SpaCy
doc = nlp(text)

# Extract all dates (DATE entities)
dates = [ent.text for ent in doc.ents if ent.label_ == "DATE"]

# Remove duplicates while preserving order
unique_dates = list(dict.fromkeys(dates))

# Print all unique dates
print("Dates found in Romeo and Juliet:")
for date in unique_dates:
    print(date)


"""ğŸ§  So... how does this actually work?
1ï¸âƒ£ spacy.load("en_core_web_sm")
This loads a pretrained NLP pipeline, which includes:

Component	Role
Tokenizer	Splits text into tokens (words/punct)
Tagger	Assigns POS tags (noun, verb, etc.)
Parser	Builds syntactic structure
NER	Identifies named entities

en_core_web_sm is a small English model trained on news, web, and Wikipedia-style text.

2ï¸âƒ£ doc = nlp("Tony Kross visited Athens in August 2012.")
This runs your text through the full pipeline:

Tokenization

POS tagging

Dependency parsing

NER

â¡ï¸ Behind the scenes, NER uses a trained statistical model (neural network) that:

Sees word patterns

Uses context

Applies learned entity boundaries and labels

ğŸ” So how does it know?
Letâ€™s look at whatâ€™s learned:

â†’ "Tony Kross" â†’ PERSON
Capitalized, proper noun

Common name patterns (like first + last)

Appears near verbs like â€œvisitedâ€ â†’ likely subject

â†’ "Athens" â†’ GPE (Geopolitical Entity)
It knows from training that "Athens" is often a city name

Appears after verbs like â€œvisitedâ€ â€” again, context helps

â†’ "August 2012" â†’ DATE
Recognized as a temporal phrase

Pattern matching like [Month Year]

Learned via examples like "in May 2020", "on July 4" etc.

ğŸ“ How was it trained?
The NER model was trained on labeled examples like:

plaintext
Copy
Edit
"Barack Obama was born in Hawaii."  
â†’ [('Barack Obama', 'PERSON'), ('Hawaii', 'GPE')]
The model:

Learns what spans are entities

And what types (PERSON, ORG, DATE, GPE, etc.)

ğŸ§˜ TL;DR: What does what?
Code Part	Role
spacy.load("en_core_web_sm")	Loads pretrained model with tokenizer + NER
nlp(text)	Runs the full NLP pipeline
doc.ents	Gives you the named entities detected
ent.text, ent.label_	The entity string and its type (like DATE)

ğŸ” Want to peek under the hood?
We can:

Visualize NER with spacy.displacy

List all entity types supported (like ORG, MONEY, NORP)

Compare what a larger model (like en_core_web_trf) sees differently

Just say the word! ğŸ˜„







You said:
interesting. but that:  trained statistical model (neural network) is again sth else than the dimensions thingi?


ChatGPT said:
Fantastic connection, Lee â€” and yes! You're right to ask this.

Let me break it down crystal clear:

ğŸ§  Whatâ€™s the difference?
ğŸ”¸ Word Embeddings (like those 768 dimensions)
These are vectors: they represent the meaning of a word/token

They live inside a matrix and are learned during training

Example: "king" â†’ [0.2, -1.3, ..., 0.9] (768D in GPT-2)

Used as input to a neural network

â¡ï¸ Think of them like:

â€œHow we represent the words mathematicallyâ€

ğŸ”¸ Neural Network (like spaCy's NER model)
A system of layers and weights that takes those vectors and learns patterns

Example: "Tony Kross" in context â†’ the model classifies it as a PERSON

The network looks at the sequence, position, context, and uses learned weights to make predictions

â¡ï¸ Think of it like:

â€œThe brain that makes decisions based on those vectorsâ€

ğŸ§¬ How they work together
Letâ€™s walk through what happens inside spaCy (or any NLP model):

"Tony" â†’ vector A
"Kross" â†’ vector B
"visited" â†’ vector C
etc...

These vectors (like 300D or 768D) are passed into a neural network

The network processes them â€” using layers, weights, and activation functions â€” and says:

â†’ "Tony Kross" = PERSON
â†’ "Athens" = GPE

ğŸ§˜ TL;DR:
Concept	What it is	Analogy
Embedding	A vector of numbers (meaning of word)	How you represent a word numerically
Neural Network	A model that takes vectors as input and makes decisions	A brain processing information

âœ… Word embeddings are input data
âœ… Neural networks are the decision-making machine"""